{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary \n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "veri setini birleştirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data organized successfully.\n"
     ]
    }
   ],
   "source": [
    "def organize_data(base_dir, target_dir):\n",
    "    # Create the target directories if they do not exist\n",
    "    colour_dir = os.path.join(target_dir, 'colour')\n",
    "    grey_dir = os.path.join(target_dir, 'grey')\n",
    "    os.makedirs(colour_dir, exist_ok=True)\n",
    "    os.makedirs(grey_dir, exist_ok=True)\n",
    "\n",
    "    # Define the source directories\n",
    "    cars_colour = os.path.join(base_dir, 'Cars', 'cars_colour')\n",
    "    cars_gray = os.path.join(base_dir, 'Cars', 'cars_grey')\n",
    "    flowers_colour = os.path.join(base_dir, 'Flowers', 'flowers_colour')\n",
    "    flowers_gray = os.path.join(base_dir, 'Flowers', 'flowers_grey')\n",
    "\n",
    "    # Copy colour images to the colour directory\n",
    "    for src_dir in [cars_colour, flowers_colour]:\n",
    "        for filename in os.listdir(src_dir):\n",
    "            src_file = os.path.join(src_dir, filename)\n",
    "            dst_file = os.path.join(colour_dir, filename)\n",
    "            shutil.copyfile(src_file, dst_file)\n",
    "\n",
    "    # Copy gray images to the gray directory\n",
    "    for src_dir in [cars_gray, flowers_gray]:\n",
    "        for filename in os.listdir(src_dir):\n",
    "            src_file = os.path.join(src_dir, filename)\n",
    "            dst_file = os.path.join(grey_dir, filename)\n",
    "            shutil.copyfile(src_file, dst_file)\n",
    "\n",
    "    print(\"Data organized successfully.\")\n",
    "\n",
    "# Base directory containing the 'cars' and 'flowers' directories\n",
    "base_directory = '../../Downloads/data/'\n",
    "target_dir = './data/'\n",
    "organize_data(base_directory, target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri setini train, val, test için bölme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data organized successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_directories(base_dir):\n",
    "    for subset in ['train', 'validation', 'test']:\n",
    "        for category in ['colour', 'grey']:\n",
    "            os.makedirs(os.path.join(base_dir, subset, category), exist_ok=True)\n",
    "\n",
    "def split_and_copy_files(base_dir, filenames, train_ratio=0.7, validation_ratio=0.2):\n",
    "    train_files, temp_files = train_test_split(filenames, test_size=(1 - train_ratio))\n",
    "    validation_files, test_files = train_test_split(temp_files, test_size=(1 - validation_ratio / (1 - train_ratio)))\n",
    "\n",
    "    return train_files, validation_files, test_files\n",
    "\n",
    "def copy_files(file_list, src_dir, dst_dir):\n",
    "    for filename in file_list:\n",
    "        shutil.copyfile(os.path.join(src_dir, filename), os.path.join(dst_dir, filename))\n",
    "\n",
    "def organize_data(base_dir):\n",
    "    colour_dir = os.path.join(base_dir, 'colour')\n",
    "    grey_dir = os.path.join(base_dir, 'grey')\n",
    "\n",
    "    # List all filenames (assuming the filenames in colour and gray folders are identical)\n",
    "    filenames = os.listdir(colour_dir)\n",
    "\n",
    "    # Split the filenames into train, validation, and test sets\n",
    "    train_files, validation_files, test_files = split_and_copy_files(base_dir, filenames)\n",
    "\n",
    "    # Create the necessary directories\n",
    "    create_directories(base_dir)\n",
    "\n",
    "    # Copy files to the respective directories\n",
    "    for subset, file_list in zip(['train', 'validation', 'test'], [train_files, validation_files, test_files]):\n",
    "        copy_files(file_list, colour_dir, os.path.join(base_dir, subset, 'colour'))\n",
    "        copy_files(file_list, grey_dir, os.path.join(base_dir, subset, 'grey'))\n",
    "\n",
    "    print(\"Data organized successfully.\")\n",
    "    \n",
    "\n",
    "# Base directory containing the 'colour' and 'gray' directories\n",
    "base_directory = './data'\n",
    "organize_data(base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_pairs = []\n",
    "\n",
    "        grey_dir = os.path.join(root_dir, \"grey\")\n",
    "        color_dir = os.path.join(root_dir, \"colour\")\n",
    "\n",
    "        grey_images = sorted(os.listdir(grey_dir))\n",
    "        color_images = sorted(os.listdir(color_dir))\n",
    "\n",
    "        for grey_img, color_img in zip(grey_images, color_images):\n",
    "            self.image_pairs.append((os.path.join(grey_dir, grey_img), os.path.join(color_dir, color_img)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grey_img_path, color_img_path = self.image_pairs[idx]\n",
    "        grey_image = Image.open(grey_img_path).convert('RGB')  # Gri tonlamalı görüntü RGB formatına dönüştürülür\n",
    "        color_image = Image.open(color_img_path).convert('RGB')  # Renkli görüntü\n",
    "\n",
    "        if self.transform:\n",
    "            grey_image = self.transform(grey_image)\n",
    "            color_image = self.transform(color_image)\n",
    "\n",
    "        return grey_image, color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_path = './data/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = ColorizationDataset(root_dir=os.path.join(workspace_path, \"train\"), transform=transform)\n",
    "val_dataset = ColorizationDataset(root_dir=os.path.join(workspace_path, \"validation\"), transform=transform)\n",
    "test_dataset = ColorizationDataset(root_dir=os.path.join(workspace_path, \"test\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets, tolerance=0.1):\n",
    "    difference = torch.abs(outputs - targets)\n",
    "    correct = (difference < tolerance).float().mean()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.0670, Train Accuracy: 0.2586, Validation Loss: 0.0601, Validation Accuracy: 0.3037\n",
      "Epoch [2/50], Train Loss: 0.0479, Train Accuracy: 0.3499, Validation Loss: 0.0575, Validation Accuracy: 0.2677\n",
      "Epoch [3/50], Train Loss: 0.0390, Train Accuracy: 0.3716, Validation Loss: 0.0249, Validation Accuracy: 0.5717\n",
      "Epoch [4/50], Train Loss: 0.0269, Train Accuracy: 0.5558, Validation Loss: 0.0283, Validation Accuracy: 0.5385\n",
      "Epoch [5/50], Train Loss: 0.0262, Train Accuracy: 0.5474, Validation Loss: 0.0256, Validation Accuracy: 0.5554\n",
      "Epoch [6/50], Train Loss: 0.0230, Train Accuracy: 0.5797, Validation Loss: 0.0232, Validation Accuracy: 0.5986\n",
      "Epoch [7/50], Train Loss: 0.0206, Train Accuracy: 0.6309, Validation Loss: 0.0208, Validation Accuracy: 0.6275\n",
      "Epoch [8/50], Train Loss: 0.0192, Train Accuracy: 0.6479, Validation Loss: 0.0197, Validation Accuracy: 0.6515\n",
      "Epoch [9/50], Train Loss: 0.0182, Train Accuracy: 0.6800, Validation Loss: 0.0185, Validation Accuracy: 0.6751\n",
      "Epoch [10/50], Train Loss: 0.0170, Train Accuracy: 0.6962, Validation Loss: 0.0181, Validation Accuracy: 0.6828\n",
      "Epoch [11/50], Train Loss: 0.0164, Train Accuracy: 0.7050, Validation Loss: 0.0170, Validation Accuracy: 0.6989\n",
      "Epoch [12/50], Train Loss: 0.0159, Train Accuracy: 0.7155, Validation Loss: 0.0167, Validation Accuracy: 0.6875\n",
      "Epoch [13/50], Train Loss: 0.0155, Train Accuracy: 0.7238, Validation Loss: 0.0162, Validation Accuracy: 0.7110\n",
      "Epoch [14/50], Train Loss: 0.0150, Train Accuracy: 0.7324, Validation Loss: 0.0154, Validation Accuracy: 0.7263\n",
      "Epoch [15/50], Train Loss: 0.0147, Train Accuracy: 0.7365, Validation Loss: 0.0153, Validation Accuracy: 0.7307\n",
      "Epoch [16/50], Train Loss: 0.0143, Train Accuracy: 0.7488, Validation Loss: 0.0149, Validation Accuracy: 0.7389\n",
      "Epoch [17/50], Train Loss: 0.0140, Train Accuracy: 0.7523, Validation Loss: 0.0147, Validation Accuracy: 0.7354\n",
      "Epoch [18/50], Train Loss: 0.0140, Train Accuracy: 0.7505, Validation Loss: 0.0160, Validation Accuracy: 0.7191\n",
      "Epoch [19/50], Train Loss: 0.0141, Train Accuracy: 0.7495, Validation Loss: 0.0148, Validation Accuracy: 0.7436\n",
      "Epoch [20/50], Train Loss: 0.0137, Train Accuracy: 0.7536, Validation Loss: 0.0143, Validation Accuracy: 0.7443\n",
      "Epoch [21/50], Train Loss: 0.0136, Train Accuracy: 0.7538, Validation Loss: 0.0144, Validation Accuracy: 0.7446\n",
      "Epoch [22/50], Train Loss: 0.0133, Train Accuracy: 0.7608, Validation Loss: 0.0140, Validation Accuracy: 0.7488\n",
      "Epoch [23/50], Train Loss: 0.0130, Train Accuracy: 0.7660, Validation Loss: 0.0139, Validation Accuracy: 0.7534\n",
      "Epoch [24/50], Train Loss: 0.0129, Train Accuracy: 0.7695, Validation Loss: 0.0138, Validation Accuracy: 0.7528\n",
      "Epoch [25/50], Train Loss: 0.0129, Train Accuracy: 0.7645, Validation Loss: 0.0138, Validation Accuracy: 0.7556\n",
      "Epoch [26/50], Train Loss: 0.0127, Train Accuracy: 0.7735, Validation Loss: 0.0135, Validation Accuracy: 0.7643\n",
      "Epoch [27/50], Train Loss: 0.0126, Train Accuracy: 0.7784, Validation Loss: 0.0140, Validation Accuracy: 0.7498\n",
      "Epoch [28/50], Train Loss: 0.0128, Train Accuracy: 0.7702, Validation Loss: 0.0135, Validation Accuracy: 0.7538\n",
      "Epoch [29/50], Train Loss: 0.0127, Train Accuracy: 0.7766, Validation Loss: 0.0133, Validation Accuracy: 0.7664\n",
      "Epoch [30/50], Train Loss: 0.0124, Train Accuracy: 0.7742, Validation Loss: 0.0132, Validation Accuracy: 0.7668\n",
      "Epoch [31/50], Train Loss: 0.0123, Train Accuracy: 0.7804, Validation Loss: 0.0134, Validation Accuracy: 0.7678\n",
      "Epoch [32/50], Train Loss: 0.0123, Train Accuracy: 0.7823, Validation Loss: 0.0134, Validation Accuracy: 0.7637\n",
      "Epoch [33/50], Train Loss: 0.0123, Train Accuracy: 0.7800, Validation Loss: 0.0132, Validation Accuracy: 0.7678\n",
      "Epoch [34/50], Train Loss: 0.0122, Train Accuracy: 0.7819, Validation Loss: 0.0130, Validation Accuracy: 0.7745\n",
      "Epoch [35/50], Train Loss: 0.0121, Train Accuracy: 0.7860, Validation Loss: 0.0130, Validation Accuracy: 0.7636\n",
      "Epoch [36/50], Train Loss: 0.0123, Train Accuracy: 0.7727, Validation Loss: 0.0129, Validation Accuracy: 0.7766\n",
      "Epoch [37/50], Train Loss: 0.0120, Train Accuracy: 0.7877, Validation Loss: 0.0134, Validation Accuracy: 0.7734\n",
      "Epoch [38/50], Train Loss: 0.0121, Train Accuracy: 0.7796, Validation Loss: 0.0129, Validation Accuracy: 0.7712\n",
      "Epoch [39/50], Train Loss: 0.0120, Train Accuracy: 0.7912, Validation Loss: 0.0130, Validation Accuracy: 0.7705\n",
      "Epoch [40/50], Train Loss: 0.0120, Train Accuracy: 0.7858, Validation Loss: 0.0130, Validation Accuracy: 0.7680\n",
      "Epoch [41/50], Train Loss: 0.0119, Train Accuracy: 0.7838, Validation Loss: 0.0127, Validation Accuracy: 0.7885\n",
      "Epoch [42/50], Train Loss: 0.0117, Train Accuracy: 0.7969, Validation Loss: 0.0127, Validation Accuracy: 0.7682\n",
      "Epoch [43/50], Train Loss: 0.0119, Train Accuracy: 0.7824, Validation Loss: 0.0128, Validation Accuracy: 0.7801\n",
      "Epoch [44/50], Train Loss: 0.0119, Train Accuracy: 0.7930, Validation Loss: 0.0128, Validation Accuracy: 0.7747\n",
      "Epoch [45/50], Train Loss: 0.0118, Train Accuracy: 0.7867, Validation Loss: 0.0128, Validation Accuracy: 0.7817\n",
      "Epoch [46/50], Train Loss: 0.0116, Train Accuracy: 0.7972, Validation Loss: 0.0124, Validation Accuracy: 0.7781\n",
      "Epoch [47/50], Train Loss: 0.0114, Train Accuracy: 0.7925, Validation Loss: 0.0124, Validation Accuracy: 0.7876\n",
      "Epoch [48/50], Train Loss: 0.0114, Train Accuracy: 0.7958, Validation Loss: 0.0123, Validation Accuracy: 0.7881\n",
      "Epoch [49/50], Train Loss: 0.0114, Train Accuracy: 0.8001, Validation Loss: 0.0123, Validation Accuracy: 0.7774\n",
      "Epoch [50/50], Train Loss: 0.0113, Train Accuracy: 0.7916, Validation Loss: 0.0123, Validation Accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ColorizationNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for grey_images, color_images in train_loader:\n",
    "        grey_images, color_images = grey_images.to(device), color_images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(grey_images)\n",
    "        loss = criterion(outputs, color_images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * grey_images.size(0)\n",
    "        train_accuracy += calculate_accuracy(outputs, color_images).item() * grey_images.size(0)\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = train_accuracy / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for grey_images, color_images in val_loader:\n",
    "            grey_images, color_images = grey_images.to(device), color_images.to(device)\n",
    "            outputs = model(grey_images)\n",
    "            loss = criterion(outputs, color_images)\n",
    "            val_loss += loss.item() * grey_images.size(0)\n",
    "            val_accuracy += calculate_accuracy(outputs, color_images).item() * grey_images.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_accuracy = val_accuracy / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0098, Test Accuracy: 0.7990\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_accuracy = 0.0\n",
    "with torch.no_grad():\n",
    "    for grey_images, color_images in test_loader:\n",
    "        grey_images, color_images = grey_images.to(device), color_images.to(device)\n",
    "        outputs = model(grey_images)\n",
    "        loss = criterion(outputs, color_images)\n",
    "        test_loss += loss.item() * grey_images.size(0)\n",
    "        test_accuracy += calculate_accuracy(outputs, color_images).item() * grey_images.size(0)\n",
    "\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "test_accuracy = test_accuracy / len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchimg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
